{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Part 3: Pre-Processing the data)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "•\tAssigning numerical values to categorical data;\n",
        "•\tHandling missing values; and\n",
        "•\tNormalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load data and essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load libraries for data processing\n",
        "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# visualization\n",
        "import seaborn as sns \n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8,4) \n",
        "#plt.rcParams['axes.titlesize'] = 'large'\n",
        "\n",
        "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
        "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
        "df.head(3)\n"
      ]
    },
    {
      "source": [
        "Label encoding\n",
        "\n",
        "Here, I assign the 30 features to a NumPy array X, and transform the class labels from their original string representation (M and B) into integers"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Assign predictors to a variable of ndarray (matrix) type\n",
        "array = df.values\n",
        "X = array[:,1:31]\n",
        "y = array[:,0]\n",
        "X\n"
      ]
    },
    {
      "source": [
        "After encoding the class labels(diagnosis) in an array y, the malignant tumors are now represented as class 1(i.e prescence of cancer cells) and the benign tumors are represented as class 0 (i.e no cancer cells detection), respectively, illustrated by calling the transform method of LabelEncorder on two dummy variables.**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#transform the class labels from their original string representation (M and B) into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y\n",
        "\n",
        "# Call the transform method of LabelEncorder on two dummy variables\n",
        "# le.transform (['M', 'B'])\n"
      ]
    },
    {
      "source": [
        "# Assesing Model Accuracy: Split data into training and test sets\n",
        "\n",
        "The simplest method to evaluate the performance of a machine learning algorithm is to use different training and testing datasets. Here I will\n",
        "•\tSplit the available data into a training set and a testing set. (70% training, 30% test)\n",
        "•\tTrain the algorithm on the first part,\n",
        "•\tmake predictions on the second part and\n",
        "•\tevaluate the predictions against the expected results.\n",
        "The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "##Split data set in train 70% and test 30%\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=7)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
      ]
    },
    {
      "source": [
        "# Feature Standardization\n",
        "\n",
        "Let’s evaluate the same algorithms with a standardized copy of the dataset. Here, I use sklearn to scale and transform the data such that each attribute has a mean value of zero and a standard deviation of one"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the  data (center around 0 and scale to remove the variance).\n",
        "scaler =StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n"
      ]
    },
    {
      "source": [
        "# Feature decomposition using Principal Component Analysis (PCA)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# feature extraction\n",
        "pca = PCA(n_components=10)\n",
        "fit = pca.fit(Xs)\n",
        "\n",
        "# summarize components\n",
        "#print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
        "#print(fit.components_)\n",
        "In [35]:\n",
        "X_pca = pca.transform(Xs)\n",
        "\n",
        "PCA_df = pd.DataFrame()\n",
        "\n",
        "PCA_df['PCA_1'] = X_pca[:,0]\n",
        "PCA_df['PCA_2'] = X_pca[:,1]\n",
        "\n",
        "plt.plot(PCA_df['PCA_1'][df.diagnosis == 'M'],PCA_df['PCA_2'][df.diagnosis == 'M'],'o', alpha = 0.7, color = 'r')\n",
        "plt.plot(PCA_df['PCA_1'][df.diagnosis == 'B'],PCA_df['PCA_2'][df.diagnosis == 'B'],'o', alpha = 0.7, color = 'b')\n",
        "\n",
        "plt.xlabel('PCA_1')\n",
        "plt.ylabel('PCA_2')\n",
        "plt.legend(['Malignant','Benign'])\n",
        "plt.show()\n"
      ]
    },
    {
      "source": [
        "Now, what we got after applying the linear PCA transformation is a lower dimensional subspace (from 3D to 2D in this case), where the samples are most spread along the new feature axes."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The amount of variance that each PC explains\n",
        "var= pca.explained_variance_ratio_\n",
        "#Cumulative Variance explains\n",
        "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
        "#print(var1)\n"
      ]
    },
    {
      "source": [
        "In order to decide how many principal components should be retained, it is common to summarise the results of a principal components analysis by making a scree plot. "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The amount of variance that each PC explains\n",
        "var= pca.explained_variance_ratio_\n",
        "#Cumulative Variance explains\n",
        "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
        "#print(var1)\n",
        "\n",
        "plt.plot(var)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Eigenvalue')\n",
        "\n",
        "leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3, shadow=False, markerscale=0.4)\n",
        "leg.get_frame().set_alpha(0.4)\n",
        "leg.set_draggable(state=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "source": [
        "# Observation\n",
        "\n",
        "The most obvious change in slope in the scree plot occurs at component 2, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that the first three components should be retained.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Summary: Data Preprocing Approach used\n",
        "\n",
        "*\tassign features to a NumPy array X, and transform the class labels from their original string representation (M and B) into integers.\n",
        "*\tSplit data into training and test sets\n",
        "*\tStandardize the data.\n",
        "*\tObtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix\n",
        "*\tSort eigenvalues in descending order and choose the kk eigenvectors that correspond to the kk largest eigenvalues where k is the number of dimensions of the new feature subspace (�≤��≤�).\n",
        "*\tConstruct the projection matrix W from the selected k eigenvectors.\n",
        "*\tTransform the original dataset X via W to obtain a k-dimensional feature subspace Y\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}