{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Part_5: Optimizing the SVM Classifier)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Machine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this part, we'll aim to tune parameters of the SVM Classification model using scikit-learn. \n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load libraries for data processing\n",
        "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "## Supervised learning.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# visualization\n",
        "import seaborn as sns \n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8,4) \n"
      ]
    },
    {
      "source": [
        "# Build a predictive model and evaluate with 5-cross validation using support vector classifies (ref Part_4) for details"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('outputs/datasets/cleaned', index_col=False)\n",
        "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
        "\n",
        "# Assign predictors to a variable of ndarray (matrix) type\n",
        "array = df.values\n",
        "X = array[:,1:31]\n",
        "y = array[:,0]\n",
        "\n",
        "# Transform the class labels from their original string representation (M and B) into integers\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Normalize the  data (center around 0 and scale to remove the variance).\n",
        "scaler =StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# feature extraction\n",
        "pca = PCA(n_components=10)\n",
        "fit = pca.fit(Xs)\n",
        "X_pca = pca.transform(Xs)\n",
        "\n",
        "# Divide records in training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=2, stratify=y)\n",
        "\n",
        "# Create an SVM classifier and train it on 70% of the data set.\n",
        "clf = SVC(probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Analyze accuracy of predictions on 30% of the holdout test sample.\n",
        "classifier_score = clf.score(X_test, y_test)\n",
        "print ('\\nThe classifier accuracy score is {:03.2f}\\n'.format(classifier_score))\n",
        "\n",
        "clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True))\n",
        "scores = cross_val_score(clf2, X_pca, y, cv=3)\n",
        "\n",
        "# Get average of 5-fold cross-validation score using an SVC estimator.\n",
        "n_folds = 5\n",
        "cv_error = np.average(cross_val_score(SVC(), X_pca, y, cv=n_folds))\n",
        "#print ('\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error))\n",
        "\n",
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred ))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
        "for i in range(cm.shape[0]):\n",
        "     for j in range(cm.shape[1]):\n",
        "         ax.text(x=j, y=i,\n",
        "                s=cm[i, j], \n",
        "                va='center', ha='center')\n",
        "plt.xlabel('Predicted Values', )\n",
        "plt.ylabel('Actual Values')\n",
        "plt.show()\n"
      ]
    },
    {
      "source": [
        "# Importance of optimizing a classifier"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "We can tune two key parameters of the SVM algorithm:\n",
        "\n",
        "•\tthe value of C (how much to relax the margin)\n",
        "\n",
        "•\tand the type of kernel.\n",
        "\n",
        "The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\n",
        "Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
        "\n",
        "•\tGrid Search Parameter Tuning.\n",
        "\n",
        "•\tRandom Search Parameter Tuning.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train classifiers.\n",
        "kernel_values = [ 'linear' ,  'poly' ,  'rbf' ,  'sigmoid' ]\n",
        "param_grid = {'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6),'kernel': kernel_values}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"The best parameters are %s with a score of %0.2f\"\n",
        "      % (grid.best_params_, grid.best_score_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.best_estimator_.probability = True\n",
        "clf = grid.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "#print(cm)\n",
        "print(classification_report(y_test, y_pred ))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
        "for i in range(cm.shape[0]):\n",
        "     for j in range(cm.shape[1]):\n",
        "         ax.text(x=j, y=i,\n",
        "                s=cm[i, j], \n",
        "                va='center', ha='center')\n",
        "plt.xlabel('Predicted Values', )\n",
        "plt.ylabel('Actual Values')\n",
        "plt.show()\n"
      ]
    },
    {
      "source": [
        "# Decision boundaries of different classifiers\n",
        "\n",
        "Let's see the decision boundaries produced by the linear, Gaussian and polynomial classifiers."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "def decision_plot(X_train, y_train, n_neighbors, weights):\n",
        "       h = .02  # step size in the mesh\n",
        "\n",
        "Xtrain = X_train[:, :2] # we only take the first two features.\n",
        "\n",
        "#================================================================\n",
        "# Create color maps\n",
        "#================================================================\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "\n",
        "\n",
        "#================================================================\n",
        "# we create an instance of SVM and fit out data. \n",
        "# We do not scale ourdata since we want to plot the support vectors\n",
        "#================================================================\n",
        "\n",
        "C = 1.0  # SVM regularization parameter\n",
        "\n",
        "svm = SVC(kernel='linear', random_state=0, gamma=0.1, C=C).fit(Xtrain, y_train)\n",
        "rbf_svc = SVC(kernel='rbf', gamma=0.7, C=C).fit(Xtrain, y_train)\n",
        "poly_svc = SVC(kernel='poly', degree=3, C=C).fit(Xtrain, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15, 9) \n",
        "plt.rcParams['axes.titlesize'] = 'large'\n",
        "    \n",
        "    # create a mesh to plot in\n",
        "x_min, x_max = Xtrain[:, 0].min() - 1, Xtrain[:, 0].max() + 1\n",
        "y_min, y_max = Xtrain[:, 1].min() - 1, Xtrain[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, clf in enumerate((svm, rbf_svc, poly_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
        "    plt.xlabel('radius_mean')\n",
        "    plt.ylabel('texture_mean')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "source": [
        "# Conclusion\n",
        "\n",
        "This work demonstrates the modelling of breast cancer as classification task using Support Vector Machine\n",
        "The SVM performs better when the dataset is standardized so that all attributes have a mean value of zero and a standard deviation of one. We can calculate this from the entire training dataset and apply the same transform to the input attributes from the validation dataset.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}