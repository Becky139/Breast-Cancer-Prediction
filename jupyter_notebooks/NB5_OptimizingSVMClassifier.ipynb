{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the SVM Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "Machine learning models are parameterized so that their behaviour can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this part, we'll aim to tune parameters of the SVM Classification model using scikit-learn. \n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/cleaned/data.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* outputs/nb5/classification_report.jpeg, outputs/nb5/grid_svc_classification.jpeg, outputs/nb5/svc_charts.jpeg\n",
    "* accuracy_report.png, grid_svc_accuracy.png\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are parameterized so that their behaviour can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this notebook, I aim to tune parameters of the SVM Classification model using scikit-learn. \n",
    "\n",
    "#### Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load libraries for data processing\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Supervised learning.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "data = pd.read_csv(\"outputs/datasets/cleaned/data.csv\", index_col=False)\n",
    "data.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a predictive model and evaluate with 5-cross validation using support vector classifies (ref NB4) for details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assign predictors to a variable of ndarray (matrix) type\n",
    "array = data.values\n",
    "X = array[:, 1:31]\n",
    "y = array[:, 0]\n",
    "\n",
    "# Transform the class labels from their original string representation\n",
    "# (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=10)\n",
    "fit = pca.fit(Xs)\n",
    "X_pca = pca.transform(Xs)\n",
    "\n",
    "# Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.3, random_state=2, stratify=y\n",
    ")\n",
    "\n",
    "# Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, y_test)\n",
    "print(\"\\nThe classifier accuracy score is {:03.2f}\\n\".format(classifier_score))\n",
    "\n",
    "clf2 = make_pipeline(SelectKBest(f_regression, k=3), SVC(probability=True))\n",
    "scores = cross_val_score(clf2, X_pca, y, cv=3)\n",
    "\n",
    "# Get average of 5-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 5\n",
    "cv_error = np.average(cross_val_score(SVC(), X_pca, y, cv=n_folds))\n",
    "print(\n",
    "    \"\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n\".format(\n",
    "        n_folds, cv_error\n",
    "    )\n",
    ")\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va=\"center\", ha=\"center\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.savefig(\"outputs/nb5/classification_report.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of optimizing a classifier\n",
    "\n",
    "We can tune two key parameters of the SVM algorithm:\n",
    "* the value of C (how much to relax the margin) \n",
    "* and the type of kernel. \n",
    "\n",
    "The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\n",
    "\n",
    "Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
    " * Grid Search Parameter Tuning. \n",
    " * Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train classifiers.\n",
    "kernel_values = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 2, 6),\n",
    "    \"gamma\": np.logspace(-3, 2, 6),\n",
    "    \"kernel\": kernel_values,\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.probability = True\n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(cm)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va=\"center\", ha=\"center\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.savefig(\"outputs/nb5/grid_svc_classification.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision boundaries of different classifiers\n",
    "Let's see the decision boundaries produced by the linear, Gaussian and polynomial classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "def decision_plot(X_train, y_train, n_neighbors, weights):\n",
    "    h = 0.02  # step size in the mesh\n",
    "\n",
    "\n",
    "Xtrain = X_train[:, :2]  # we only take the first two features.\n",
    "\n",
    "# ================================================================\n",
    "# Create color maps\n",
    "# ================================================================\n",
    "cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
    "cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# we create an instance of SVM and fit out data.\n",
    "# We do not scale ourdata since we want to plot the support vectors\n",
    "# ================================================================\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "svm = SVC(kernel=\"linear\", random_state=0, gamma=0.1, C=C).fit(Xtrain, y_train)\n",
    "rbf_svc = SVC(kernel=\"rbf\", gamma=0.7, C=C).fit(Xtrain, y_train)\n",
    "poly_svc = SVC(kernel=\"poly\", degree=3, C=C).fit(Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9) \n",
    "plt.rcParams['axes.titlesize'] = 'large'\n",
    "    \n",
    "    # create a mesh to plot in\n",
    "x_min, x_max = Xtrain[:, 0].min() - 1, Xtrain[:, 0].max() + 1\n",
    "y_min, y_max = Xtrain[:, 1].min() - 1, Xtrain[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, clf in enumerate((svm, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel(\"radius_mean\")\n",
    "    plt.ylabel(\"texture_mean\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.savefig(\"outputs/nb5/svc_charts.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This work demonstrates the modelling of breast cancer as classification task using Support Vector Machine \n",
    "\n",
    "The SVM performs better when the dataset is standardized so that all attributes have a mean value of zero and a standard deviation of one. We can calculate this from the entire training dataset and apply the same transform to the input attributes from the validation dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Task:\n",
    "1. Summary and conclusion of findings\n",
    "2. Compare with other classification methods\n",
    "   * Decision trees with tree.DecisionTreeClassifier();\n",
    "   * K-nearest neighbours with neighbors.KNeighborsClassifier();\n",
    "   * Random forests with ensemble.RandomForestClassifier();\n",
    "   * Perceptron (both gradient and stochastic gradient) with mlxtend.classifier.Perceptron; and\n",
    "   * Multilayer perceptron network (both gradient and stochastic gradient) with mlxtend.classifier.MultiLayerPerceptron.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}