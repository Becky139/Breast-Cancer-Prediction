{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Part 6 Feature Selection And Final Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write here your notebook objective, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "source": [
        "X.shape"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "As the dimension of our data at the moment is 30, it is likely that the model we create is going to fall into the limitations of the curse of dimensionality. This can be solved with the use of a technique called Feature selection. This refers to the process of selecting a subset of attributes for efficient model construction."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "#### Selecting The K Best Features for Our Dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = [] \n",
        "\n",
        "best_features = SelectKBest(chi2 , k = 5)\n",
        "\n",
        "\n",
        "fit = best_features.fit(X , Y).get_support()\n",
        "\n",
        "for bool, feature in zip(fit, dataset.columns):\n",
        "     if bool:\n",
        "        selected_features.append(feature)\n",
        "print(\"The best features are:{}\".format(selected_features)) # The list of your 5 best features"
      ]
    },
    {
      "source": [
        "#### Viewing The Scores of the Top-k Selected Features"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_scores = {}\n",
        "\n",
        "for key, value in zip(dataset.columns,best_features.fit(X , Y).scores_):\n",
        "    feature_scores[key] = value\n",
        "\n",
        "\n",
        "for key, value in feature_scores.items():\n",
        "    if key in selected_features:\n",
        "        print(f\"{key} : {value}\")"
      ]
    },
    {
      "source": [
        "#### Dropping Unnecessary columns/features from the dataset."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = dataset[[\"texture_mean\" , \"perimeter_mean\" , \"perimeter_se\" , \"texture_worst\" , \"perimeter_worst\"]]\n",
        "target = dataset[\"diagnosis\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert features.shape[0] == target.shape[0]"
      ]
    },
    {
      "source": [
        "### Final Model "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "#### Implementing KNN in our selected features using the bestfit hyper-parameters "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train , X_test , Y_train , Y_test  = train_test_split(features , target , test_size = 0.33 , random_state = 42)\n",
        "\n",
        "KNN_model = KNeighborsClassifier(n_neighbors = k).fit(X_train , Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y_pred = KNN_model.predict(X_test)\n",
        "Y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics.accuracy_score(Y_test, Y_pred)"
      ]
    },
    {
      "source": [
        "The accuracy of KNN model after hyperparameter tuning has increased."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "#### Implementing RandomForest Classifier in our selected features using the bestfit hyper-parameters "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train , X_test , Y_train , Y_test  = train_test_split(features , target , test_size = 0.33 , random_state = 42)\n",
        "\n",
        "model_rf = RandomForestClassifier()\n",
        "\n",
        "RF_Model = RandomizedSearchCV(estimator = model_rf ,param_distributions = random_search, cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\n",
        "\n",
        "RF_Model.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y_pred = RF_Model.predict(X_test)\n",
        "metrics.accuracy_score(Y_test , Y_pred)"
      ]
    },
    {
      "source": [
        "The accuracy of Random Forest Classifier has also increased after hyperparameter tuning over the selected features."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Conclusion"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "To sum up, initially a datset that consisted of the features or details related to the infected cells present in the breast were selected and understood better with the use of available python pandas library.  After that , the matplotlib and seaborn libraries were used to visually represent or map the relationship between various features present in our dataset. \n",
        "\n",
        "Now, after that the first primary models on classification algorithms such as Logistic Regression, KNN Classifier and Random Forest Classifier were built using the sklearn library and evaluated using the available metrics. \n",
        "\n",
        "After understanding the need of the cross validation for building these models, all of these primary models were rebuilt using the K-fold and Stratified Cross Validation techniques and likewise the accuracies were checked. Both the cross validation techniques , seemed to impact the accuracy of the model in an uniform manner with **Random Forest Classifier with the highest accuracy** here.\n",
        "\n",
        "Next, was the turn to pick the best fit combination of hyperparameters for our Random Forest and KNN classifiers. Therefore, RandomizedSearch CV and GridSearch CV was used respectively to find the best combination of parameters for our models for better result. \n",
        "\n",
        "Derived Best Fit Hyper Parameters for Random Forest  : \n",
        "\n",
        "{'criterion': 'entropy', 'gini'        , <br>\n",
        " 'max_depth': 5, 137, 270, 403, 536,  668, 801, 934, 1067, 1200, None , <br>\n",
        " 'max_features': 'auto', 'sqrt', 'log2',   None, <br>\n",
        "  'min_samples_leaf': 4, 6, 8, 12,<br>\n",
        " 'min_samples_split': 3, 7, 10, 14,<br>\n",
        " 'n_estimators': 5, 602, 1200 }\n",
        "                \n",
        "Derived Best Fit Hyper Parameters for KNN Classifier  : \n",
        "\n",
        "{'n_neighbors' : 4}\n",
        "\n",
        "Finally after finding the best tuning hyper parameters for the model , the curse of dimensionality was taken into consideration and the SelectKbest feature selection technique was used to select top 5 best features from the dataset based on their scores. \n",
        "\n",
        "The best features were 'texture_mean', 'perimeter_mean', 'perimeter_se', 'texture_worst' and  'perimeter_worst'.\n",
        "\n",
        "**Finally,** after the implementation of all these refinement techniques the primarily built models were rebult by plugging in the best fit hyperparameters and selected features. **Performing this demonstrated an increase in the accuracy of the models but both our classifiers that are Random Forest And KNN appear to hold the same amount of accuracy at the end.**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}